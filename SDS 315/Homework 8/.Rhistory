jz_books <- books %>%
filter(str_detect(Title, regex("j|z|J|Z")))
mean_rating_jz <- mean(jz_books$averageRating)
jz_books
mean_rating_jz
web_scaped <- read_html("https://www.goodreads.com/list/show/1360.Best_Banned_Censored_and_Challenged_Books")
title <- html_text(html_elements(web_scaped, ".bookTitle span"))
author <- html_text(html_elements(web_scaped, ".authorName span")) %>%
str_squish()
averageRating <- html_text(html_elements(web_scaped, ".minirating")) %>%
str_extract("[0-9]\\.[0-9]+") %>%
as.numeric()
books <- tibble(Title = title, Author = author, AverageRating = averageRating)
print(books, n = 10)
# Calculate number of words in each title
books <- books %>%
mutate(Word_Count = str_count(Title, "\\S+"))
# Plot the distribution of word counts
ggplot(books, aes(x = Word_Count)) +
geom_histogram(binwidth = 1, fill = "blue", color = "black") +
labs(title = "Distribution of Number of Words in Book Titles", x = "Number of Words", y = "Frequency") +
theme_minimal()
# Summary statistics
word_count_summary <- summary(books$Word_Count)
word_count_summary
?str_count
# Calculate number of words in each title
books <- books %>%
mutate(Word_Count = str_count(Title))
# Plot the distribution of word counts
ggplot(books, aes(x = Word_Count)) +
geom_histogram(binwidth = 1, fill = "blue", color = "black") +
labs(title = "Distribution of Number of Words in Book Titles", x = "Number of Words", y = "Frequency") +
theme_minimal()
# Summary statistics
word_count_summary <- summary(books$Word_Count)
word_count_summary
# Calculate number of words in each title
books <- books %>%
mutate(WordCount = str_count(Title, "\\S+"))
# Plot the distribution of word counts
ggplot(books, aes(x = Word_Count)) +
geom_histogram(binwidth = 1, fill = "blue", color = "black") +
labs(title = "Distribution of Number of Words in Book Titles", x = "Number of Words", y = "Frequency") +
theme_minimal()
# Summary statistics
word_count_summary <- summary(books$Word_Count)
word_count_summary
books <- books %>%
mutate(WordCount = str_count(Title, "\\S+"))
ggplot(books, aes(x = Word_Count)) +
geom_histogram(binwidth = 1, fill = "blue", color = "black") +
labs(title = "Distribution of Number of Words in Book Titles", x = "Number of Words", y = "Frequency") +
theme_minimal()
# Summary statistics
word_count_summary <- summary(books$Word_Count)
word_count_summary
# Calculate number of words in each title
books <- books %>%
mutate(Word_Count = str_count(Title, "\\S+"))
# Plot the distribution of word counts
ggplot(books, aes(x = Word_Count)) +
geom_histogram(binwidth = 1, fill = "blue", color = "black") +
labs(title = "Distribution of Number of Words in Book Titles", x = "Number of Words", y = "Frequency") +
theme_minimal()
# Summary statistics
word_count_summary <- summary(books$Word_Count)
word_count_summary
books <- books %>%
mutate(Word_Count = str_count(Title, "\\S+"))
# Plot the distribution of word counts
ggplot(books, aes(x = Word_Count)) +
geom_histogram(binwidth = 1, color = "black") +
labs(title = "Distribution of Number of Words in Book Titles", x = "Number of Words", y = "Frequency") +
theme_minimal()
# Summary statistics
word_count_summary <- summary(books$Word_Count)
word_count_summary
books <- books %>%
mutate(Word_Count = str_count(Title, "\\S+"))
# Plot the distribution of word counts
ggplot(books, aes(x = Word_Count)) +
geom_histogram(binwidth = 1) +
labs(title = "Distribution of Number of Words in Book Titles", x = "Number of Words", y = "Frequency") +
theme_minimal()
# Summary statistics
word_count_summary <- summary(books$Word_Count)
word_count_summary
books <- books %>%
mutate(Word_Count = str_count(Title, "\\S+"))
ggplot(books, aes(x = Word_Count)) +
geom_histogram(binwidth = 1, color = "black") +
labs(title = "Distribution of Number of Words in Book Titles", x = "Number of Words", y = "Frequency")
summary(books$Word_Count)
lastName <- ifelse(
is.na(word(author, 2)),
word(author, -1),
word(author, 2)
)
books <- tibble(Title = title, Author = author, averageRating = averageRating, LastName = lastName)
shortest_last_name <- books %>%
filter(nchar(lastName) == min(nchar(lastName))) %>%
select(Title, Author)
shortest_last_name
books <- books %>%
mutate(Word_Count = str_count(Title, "\\S+"))
ggplot(books, aes(x = Word_Count)) +
geom_histogram(binwidth = 1, color = "black") +
labs(title = "Distribution of Number of Words in Book Titles", x = "Number of Words", y = "Frequency")
summary(books$Word_Count)
#Randomly sample from a vector:
x <- c(4,6,9,10,15,20)
sample(x,1)
#Sample with replacement:
sample(x,5,replace=TRUE)
?runif
#Pull 10 random numbers from a Uniform(0,100) distribution:
runif(10,0,100)
#Pull 10 random numbers from a Uniform(0,100) distribution:
runif(10,0,100)
#Pull 10 random numbers from a Uniform(0,100) distribution:
runif(10,0,100)
#Pull 10 random numbers from a Normal(0,1) distribution:
rnorm(10,0,1)
#Pull 10 random numbers from a Normal(0,1) distribution:
rnorm(10,0,1)
#Pull 10 random numbers from a Normal(0,1) distribution:
rnorm(10,0,1)
#Pull 10 random numbers from a Normal(0,1) distribution:
rnorm(10,0,1)
#Pull 10 random numbers from a Normal(0,1) distribution:
rnorm(10,0,1)
#For reproducable results:
set.seed(56)
rnorm(10,0,1)
#For reproducable results:
set.seed(56)
rnorm(10,0,1)
#For reproducable results:
set.seed(56)
rnorm(10,0,1)
?rnorm
#Group Practice A:
rnorm(100,100,15)
#Group Practice A:
rnorm(100,100,15)
#Group Practice A:
rnorm(100,100,15)
#Group Practice A:
rnorm(100,100,15)
#Create empty vector to store outcomes:
myrolls <- numeric(0)
#"Roll" a 6-sided die 5,000 times:
for (i in 1:5000) {
x <- sample(1:6,1)
myrolls[i] <- x
}
#Plot results:
library(ggplot2)
mydata <- data.frame(myrolls)
ggplot(mydata) + geom_histogram(aes(x=myrolls))
View(mydata)
#Create empty vector to store outcomes:
myrolls <- numeric(0)
#"Roll" a 6-sided die 5,000 times:
for (i in 1:5000) {
x <- sample(1:6,1)
myrolls[i] <- x
}
#Plot results:
library(ggplot2)
mydata <- data.frame(myrolls)
ggplot(mydata) + geom_histogram(aes(x=myrolls))
#Create empty vector to store outcomes:
myrolls <- numeric(0)
#"Roll" a 6-sided die 5,000 times:
for (i in 1:5000) {
x <- sample(1:6,1)
myrolls[i] <- x
}
#Plot results:
library(ggplot2)
mydata <- data.frame(myrolls)
ggplot(mydata) + geom_histogram(aes(x=myrolls))
#Create empty vectors to store outcomes:
myrolls <- numeric(0)
my4prob <- numeric(0)
#Store relative frequency of rolling a 4:
for (i in 1:5000) {
x <- sample(1:6,1)
myrolls[i] <- x
my4prob[i] <- sum(myrolls==4)/i
}
#Plot results:
plot(1:5000, my4prob, type='l')
abline(h=1/6, lty=2, col='red')
#Create empty vectors to store outcomes:
myrolls <- numeric(0)
my4prob <- numeric(0)
#Store relative frequency of rolling a 4:
for (i in 1:5000) {
x <- sample(1:6,1)
myrolls[i] <- x
my4prob[i] <- sum(myrolls==4)/i
}
#Plot results:
plot(1:5000, my4prob, type='l')
abline(h=1/6, lty=2, col='red')
#Create empty vectors to store outcomes:
myrolls <- numeric(0)
my4prob <- numeric(0)
#Store relative frequency of rolling a 4:
for (i in 1:5000) {
x <- sample(1:6,1)
myrolls[i] <- x
my4prob[i] <- sum(myrolls==4)/i
}
#Plot results:
plot(1:5000, my4prob, type='l')
abline(h=1/6, lty=2, col='red')
#Group Practice C
#Create empty vectors to store outcomes:
myrolls <- numeric(0)
my4prob <- numeric(0)
#Store relative frequency of rolling a 4:
for (i in 1:5000) {
x <- sample(1:6,1)+sample(1:6,1)
myrolls[i] <- x
my4prob[i] <- sum(myrolls==4)/i
}
#Plot results:
plot(1:5000, my4prob, type='l')
abline(h=1/6, lty=2, col='red')
#Define door options:
doors <- c(1,2,3)
#Create empty vectors to store outcomes:
always.stay <- c()
always.switch <- c()
#Simulate probabilities of "staying" and "switching":
for (i in 1:1000){
door.car <- sample(doors, 1)
door.choose <- sample(doors, 1)
always.stay[i] <- (door.choose == door.car)
remaining.doors <- setdiff(doors, union(door.car, door.choose))
if (length(remaining.doors) == 1){
shown.door <- remaining.doors
}
else shown.door <- sample(remaining.doors, 1)
door.switch <- setdiff(doors, union(door.choose, shown.door))
always.switch[i] <- (door.switch == door.car)
}
?setdiff
library(tidyverse)
library(ggplot2)
library(rvest)
library(stringr)
web_scaped <- read_html("https://jobs.apple.com/app/en-us/profile/roles")
web_scaped <- read_html("/Users/kaustubhduddala/Downloads/odwnlawmd2FE")
title <- html_text(html_elements(web_scaped, ".profile-jobs-item__info h3"))
View(web_scaped)
library(tidyverse)
library(ggplot2)
library(rvest)
library(stringr)
web_scaped <- read_html("https://www.goodreads.com/list/show/1360.Best_Banned_Censored_and_Challenged_Books")
title <- html_text(html_elements(web_scaped, ".bookTitle span"))
author <- html_text(html_elements(web_scaped, ".authorName span")) %>%
str_squish()
averageRating <- html_text(html_elements(web_scaped, ".minirating")) %>%
str_extract("[0-9]\\.[0-9]+") %>%
as.numeric()
books <- tibble(Title = title, Author = author, AverageRating = averageRating)
print(books, n = 10)
View(web_scaped)
View(web_scaped)
library(tidyverse)
library(ggplot2)
library(rvest)
library(stringr)
web_scaped <- read_html("/Users/kaustubhduddala/Downloads/odwnlawmd2FE")
title <- html_text(html_elements(web_scaped, ".profile-jobs-item__info h3"))
web_scaped <- read_html("/Users/kaustubhduddala/Downloads/odwnlawmd2FE/Careers at Apple.html")
title <- html_text(html_elements(web_scaped, ".profile-jobs-item__info h3"))
web_scaped <- read_html("https://www.goodreads.com/list/show/1360.Best_Banned_Censored_and_Challenged_Books")
title <- html_text(html_elements(web_scaped, ".bookTitle span"))
author <- html_text(html_elements(web_scaped, ".authorName span")) %>%
str_squish()
averageRating <- html_text(html_elements(web_scaped, ".minirating")) %>%
str_extract("[0-9]\\.[0-9]+") %>%
as.numeric()
books <- tibble(Title = title, Author = author, AverageRating = averageRating)
print(books, n = 10)
web_scaped <- read_html("/Users/kaustubhduddala/Downloads/odwnlawmd2FE/Careers at Apple.html")
title <- html_text(html_elements(web_scaped, ".profile-jobs-item__info h3"))
location <- html_text(html_elements(web_scaped, ".profile-jobs-item__city div"))
table <- tibble(Title = title, Loco = location)
View(table)
View(table)
web_scaped <- read_html("/Users/kaustubhduddala/Downloads/odwnlawmd2FE/Careers at Apple2.html")
title <- html_text(html_elements(web_scaped, ".profile-jobs-item__info h3"))
location <- html_text(html_elements(web_scaped, ".profile-jobs-item__city div"))
table <- tibble(Title = title, Loco = location)
web_scaped <- read_html("/Users/kaustubhduddala/Downloads/odwnlawmd2FE/Careers at Apple3.html")
title <- html_text(html_elements(web_scaped, ".profile-jobs-item__info h3"))
location <- html_text(html_elements(web_scaped, ".profile-jobs-item__city div"))
table <- tibble(Title = title, Loco = location)
watermark_chi <- sapply(p4d_sentences, clean, freq_table = p4d_letter)
p4d_letter <- read_csv("letter_frequencies.csv")
library(ggplot2)
library(tidyverse)
library(mosaic)
library(knitr)
library(kableExtra)
p1_iron_bank_fraud <- rbinom(n = 10000, size = 2021, prob = 0.024)
p1_pvalue = mean(p1_iron_bank_fraud >= 70)
cat("p value:", p1_pvalue)
hist(p1_iron_bank_fraud, main="Distribution of simulated flagged trades under null hypothesis", xlab = "Number of flagged trades")
p2_health <- rbinom(n = 100000, size = 50, prob = 0.03)
p2_pvalue = mean(p2_health >= 8)
cat("P value:", p2_pvalue)
hist(p2_health, main="Distribution of simulated health code violations under null hypothesis", xlab = "Number of health code violations")
observed_counts <- c(85, 56, 59, 27, 13)
total_jurors <- sum(observed_counts)
expected_proportions <- c(0.30, 0.25, 0.20, 0.15, 0.10)
expected_counts <- expected_proportions * total_jurors
chi_squared_test <- chisq.test(observed_counts, p = expected_proportions)
print(chi_squared_test)
p3_pvalue <- chi_squared_test$p.value
cat("P-value:", p3_pvalue, "\n")
p4d_letter <- read_csv("letter_frequencies.csv")
p4d_brown <- readLines("brown_sentences.txt")
clean <- function(text, freq_table) {
word <- str_replace_all(text, "[^A-Za-z]", "") %>% toupper()
obs_val <- table(factor(strsplit(word, "")[[1]], levels = freq_table$Letter))
total <- sum(obs_val)
expect_val <- total * freq_table$Probability
chi_test <- sum((obs_val - expect_val)^2 / expect_val)
return(chi_test)
}
p4d_letter$Probability <- p4d_letter$Probability / sum(p4d_letter$Probability)
chi_val <- sapply(p4d_brown, clean, freq_table = p4d_letter)
hist(chi_val, main = "Distribution of Chi-squared Values for sentences", xlab = "Chi-squared Statistic", breaks = 50)
p4d_sentences <- c(
"She opened the book and started to read the first chapter, eagerly anticipating what might come next.",
"Despite the heavy rain, they decided to go for a long walk in the park, crossing the main avenue by the fountain in the center.",
"The museum’s new exhibit features ancient artifacts from various civilizations around the world.",
"He carefully examined the document, looking for any clues that might help solve the mystery.",
"The students gathered in the auditorium to listen to the guest speaker’s inspiring lecture.",
"Feeling vexed after an arduous and zany day at work, she hoped for a peaceful and quiet evening at home, cozying up after a quick dinner with some TV, or maybe a book on her upcoming visit to Auckland.",
"The chef demonstrated how to prepare a delicious meal using only locally sourced ingredients, focusing mainly on some excellent dinner recipes from Spain.",
"They watched the sunset from the hilltop, marveling at the beautiful array of colors in the sky.",
"The committee reviewed the proposal and provided many points of useful feedback to improve the project’s effectiveness.",
"Despite the challenges faced during the project, the team worked tirelessly to ensure its successful completion, resulting in a product that exceeded everyone’s expectations."
)
watermark_chi <- sapply(p4d_sentences, clean, freq_table = p4d_letter)
pvals <- sapply(watermark_chi, \(x) mean(chi_val >= x, na.rm = TRUE))
results_table <- as_tibble(data.frame(
Sentence = 1:10,
Chi_Squared = round(watermark_chi, 1),
P_value = round(pvals, digits = 3) %>% format.pval(eps = 0.0001, digits = 3)
)) %>% select(starts_with("S"):last_col())
results_table
setwd("~/Documents/UT/SDS/SDS 315/Homework 8")
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
options(scipen = 999)
library(tidyverse)
library(ggplot2)
library(kableExtra)
library(mosaic)
library(MatchIt)
data = read_csv("creatinine.csv")
data = read_csv("creatinine.csv")
crea_lm <- lm(creatclear ~ age, data = data)
beta_age <- coef(crea_lm)["age"]
beta_0 <- coef(crea_lm)["(Intercept)"]
predicted_clearance_55 <- beta_0 + beta_age * 55
predicted_clearance_55
crea_lm <- lm(creatclear ~ age, data = data)
beta_age <- coef(crea_lm)["age"]
beta_0 <- coef(crea_lm)["(Intercept)"]
predicted_clearance_55 <- beta_0 + beta_age * 55
crea_lm <- lm(creatclear ~ age, data = data)
beta_age <- coef(crea_lm)["age"]
beta_0 <- coef(crea_lm)["(Intercept)"]
predicted_clearance_55 <- beta_0 + beta_age * 55
data = read_csv("creatinine.csv")
setwd("~/Documents/UT/SDS/SDS 315/Homework 8")
setwd("~/Documents/UT/SDS/SDS 315/Homework 8")
ls
beta_age
?abs
residual_40 <- 135 - (beta_0 + beta_age * 40)
residual_60 <- 112 - (beta_0 + beta_age * 60)
residual_40 <- 135 - (beta_0 + beta_age * 40)
residual_60 <- 112 - (beta_0 + beta_age * 60)
residual_40
residual_60
covid_data <- read_csv("covid.csv")
italy_data <- covid_data %>% filter(country == "Italy")
spain_data <- covid_data %>% filter(country == "Spain")
fit_italy <- lm(log(deaths) ~ days_since_first_death, data = italy_data)
rate_italy <- coef(fit_italy)["days_since_first_death"]
italy_dbl_time <- log(2) / rate_italy
set.seed(123)
italy_boot <- do(10000) * lm(log(deaths) ~ days_since_first_death, data = resample(italy_data))
italy_rate_ci <- confint(italy_boot)
italy_dbl_ci <- confint(log(2) / italy_boot$days_since_first_death)
fit_spain <- lm(log(deaths) ~ days_since_first_death, data = spain_data)
rate_spain <- coef(fit_spain)["days_since_first_death"]
spain_dbl_time <- log(2) / rate_spain
set.seed(123)
spain_boot <- do(10000) * lm(log(deaths) ~ days_since_first_death, data = resample(spain_data))
spain_rate_ci <- confint(spain_boot)
spain_dbl_ci <- confint(log(2) / spain_boot$days_since_first_death)
ggplot(covid_data, aes(x = days_since_first_death, y = deaths, color = country)) +
geom_line() +
labs(
title = "COVID-19 Daily Deaths by Country",
x = "Days Since First Death",
y = "Number of Deaths",
color = "Country"
) +
theme_minimal()
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
options(scipen = 999)
library(tidyverse)
library(ggplot2)
library(kableExtra)
library(mosaic)
library(MatchIt)
data = read_csv("creatinine.csv")
crea_lm <- lm(creatclear ~ age, data = data)
beta_age <- coef(crea_lm)["age"]
beta_0 <- coef(crea_lm)["(Intercept)"]
predicted_clearance_55 <- beta_0 + beta_age * 55
residual_40 <- 135 - (beta_0 + beta_age * 40)
residual_60 <- 112 - (beta_0 + beta_age * 60)
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
options(scipen = 999)
library(tidyverse)
library(ggplot2)
library(kableExtra)
library(mosaic)
library(MatchIt)
data = read_csv("creatinine.csv")
crea_lm <- lm(creatclear ~ age, data = data)
beta_age <- coef(crea_lm)["age"]
beta_0 <- coef(crea_lm)["(Intercept)"]
predicted_clearance_55 <- beta_0 + beta_age * 55
residual_40 <- 135 - (beta_0 + beta_age * 40)
residual_60 <- 112 - (beta_0 + beta_age * 60)
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
options(scipen = 999)
library(tidyverse)
library(ggplot2)
library(kableExtra)
library(mosaic)
library(MatchIt)
data = read_csv("creatinine.csv")
crea_lm <- lm(creatclear ~ age, data = data)
beta_age <- coef(crea_lm)["age"]
beta_0 <- coef(crea_lm)["(Intercept)"]
predicted_clearance_55 <- beta_0 + beta_age * 55
residual_40 <- 135 - (beta_0 + beta_age * 40)
residual_60 <- 112 - (beta_0 + beta_age * 60)
covid_data <- read_csv("covid.csv")
italy_data <- covid_data %>% filter(country == "Italy")
spain_data <- covid_data %>% filter(country == "Spain")
fit_italy <- lm(log(deaths) ~ days_since_first_death, data = italy_data)
rate_italy <- coef(fit_italy)["days_since_first_death"]
italy_dbl_time <- log(2) / rate_italy
set.seed(123)
italy_boot <- do(10000) * lm(log(deaths) ~ days_since_first_death, data = resample(italy_data))
italy_rate_ci <- confint(italy_boot)
italy_dbl_ci <- confint(log(2) / italy_boot$days_since_first_death)
fit_spain <- lm(log(deaths) ~ days_since_first_death, data = spain_data)
rate_spain <- coef(fit_spain)["days_since_first_death"]
spain_dbl_time <- log(2) / rate_spain
set.seed(123)
spain_boot <- do(10000) * lm(log(deaths) ~ days_since_first_death, data = resample(spain_data))
spain_rate_ci <- confint(spain_boot)
spain_dbl_ci <- confint(log(2) / spain_boot$days_since_first_death)
ggplot(covid_data, aes(x = days_since_first_death, y = deaths, color = country)) +
geom_line() +
labs(
title = "COVID-19 Daily Deaths by Country",
x = "Days Since First Death",
y = "Number of Deaths",
color = "Country"
)
milk_data <- read_csv("milk.csv")
milk_model <- lm(log(sales) ~ log(price), data = milk_data)
elasticity_estimate <- coef(milk_model)["log(price)"]
set.seed(123)
n_boot <- 10000
boot_elasticities <- replicate(n_boot, {
sample_indices <- sample(nrow(milk_data), replace = TRUE)
boot_sample <- milk_data[sample_indices, ]
coef(lm(log(sales) ~ log(price), data = boot_sample))["log(price)"]
})
elasticity_ci <- quantile(boot_elasticities, c(0.025, 0.975))
milk_data <- read_csv("milk.csv")
milk_model <- lm(log(sales) ~ log(price), data = milk_data)
elasticity_estimate <- coef(milk_model)["log(price)"]
set.seed(123)
n_boot <- 10000
boot_elasticities <- replicate(n_boot, {
sample_indices <- sample(nrow(milk_data), replace = TRUE)
boot_sample <- milk_data[sample_indices, ]
coef(lm(log(sales) ~ log(price), data = boot_sample))["log(price)"]
})
elasticity_ci <- quantile(boot_elasticities, c(0.025, 0.975))
